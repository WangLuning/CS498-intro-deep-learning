{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import glob\n",
    "import string\n",
    "import random \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from rnn.helpers import time_since\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language recognition with an RNN\n",
    "\n",
    "If you've ever used an online translator you've probably seen a feature that automatically detects the input language. While this might be easy to do if you input unicode characters that are unique to one or a small group of languages (like \"你好\" or \"γεια σας\"), this problem is more challenging if the input only uses the available ASCII characters. In this case, something like \"těší mě\" would beome \"tesi me\" in the ascii form. This is a more challenging problem in which the language must be recognized purely by the pattern of characters rather than unique unicode characters.\n",
    "\n",
    "We will train an RNN to solve this problem for a small set of languages thta can be converted to romanized ASCII form. For training data it would be ideal to have a large and varied dataset in different language styles. However, it is easy to find copies of the Bible which is a large text translated to different languages but in the same easily parsable format, so we will use 20 different copies of the Bible as training data. Using the same book for all of the different languages will hopefully prevent minor overfitting that might arise if we used different books for each language (fitting to common characteristics of the individual books rather than the language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from unidecode import unidecode as unicodeToAscii\n",
    "\n",
    "all_characters = string.printable\n",
    "n_letters = len(all_characters)\n",
    "\n",
    "print(unicodeToAscii('těší mě'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read a file and split into lines\n",
    "def readFile(filename):\n",
    "    data = open(filename, encoding='utf-8').read().strip()\n",
    "    return unicodeToAscii(data)\n",
    "\n",
    "def get_category_data(data_path):\n",
    "    # Build the category_data dictionary, a list of names per language\n",
    "    category_data = {}\n",
    "    all_categories = []\n",
    "    for filename in glob.glob(data_path):\n",
    "        category = os.path.splitext(os.path.basename(filename))[0].split('_')[0]\n",
    "        all_categories.append(category)\n",
    "        data = readFile(filename)\n",
    "        category_data[category] = data\n",
    "    \n",
    "    return category_data, all_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original text is split into two parts, train and test, so that we can make sure that the model is not simply memorizing the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_path = 'language_data/train/*_train.txt'\n",
    "test_data_path = 'language_data/test/*_test.txt'\n",
    "\n",
    "train_category_data, all_categories = get_category_data(train_data_path)\n",
    "test_category_data, test_all_categories = get_category_data(test_data_path)\n",
    "\n",
    "n_languages = len(all_categories)\n",
    "\n",
    "print(len(all_categories))\n",
    "print(all_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1, dim=1)\n",
    "    category_i = top_i[:, 0]\n",
    "    return category_i\n",
    "\n",
    "# Turn string into long tensor\n",
    "def stringToTensor(string):\n",
    "    tensor = torch.zeros(len(string), requires_grad=True).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return tensor\n",
    "\n",
    "def load_random_batch(text, chunk_len, batch_size):\n",
    "    input_data = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    target = torch.zeros(batch_size, 1).long().to(device)\n",
    "    input_text = []\n",
    "    for i in range(batch_size):\n",
    "        category = all_categories[random.randint(0, len(all_categories) - 1)]\n",
    "        line_start = random.randint(0, len(text[category])-chunk_len)\n",
    "        category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "        line = text[category][line_start:line_start+chunk_len]\n",
    "        input_text.append(line)\n",
    "        input_data[i] = stringToTensor(line)\n",
    "        target[i] = category_tensor\n",
    "    return input_data, target, input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Model\n",
    "====================\n",
    "\n",
    "For this classification task, we can use the same model we implement for the generation task which is located in `rnn/model.py`. See the `MP4_P2_generation.ipynb` notebook for more instructions. In this case each output vector of our RNN will have the dimension of the number of possible languages (i.e. `n_languages`). We will use this vector to predict a distribution over the languages.\n",
    "\n",
    "In the generation task, we used the output of the RNN at every time step to predict the next letter and our loss included the output from each of these predictions. However, in this task we use the output of the RNN at the end of the sequence to predict the language, so our loss function will use only the predicted output from the last time step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rnn.model import RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunk_len = 50\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "n_epochs = 2000\n",
    "hidden_size = 100\n",
    "n_layers = 1\n",
    "learning_rate = 0.01\n",
    "model_type = 'rnn'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "rnn = RNN(n_letters, hidden_size, n_languages, model_type=model_type, n_layers=n_layers).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Fill in the train function. You should initialize a hidden layer representation using your RNN's `init_hidden` function, set the model gradients to zero, and loop over each time step (character) in the input tensor. For each time step compute the output of the of the RNN and the next hidden layer representation. The cross entropy loss should be computed over the last RNN output scores from the end of the sequence and the target classification tensor. Lastly, call backward on the loss and take an optimizer step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(rnn, target_tensor, data_tensor, optimizer, criterion, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - rnn: model\n",
    "    - target_target: target character data tensor of shape (batch_size, 1)\n",
    "    - data_tensor: input character data tensor of shape (batch_size, chunk_len)\n",
    "    - optimizer: rnn model optimizer\n",
    "    - criterion: loss function\n",
    "    - batch_size: data batch size\n",
    "    \n",
    "    Returns:\n",
    "    - output: output from RNN from end of sequence \n",
    "    - loss: computed loss value as python float\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    output, loss = None, None\n",
    "    \n",
    "    ####################################\n",
    "    #          YOUR CODE HERE          #\n",
    "    ####################################\n",
    "    \n",
    "    \n",
    "    ##########       END      ##########\n",
    "\n",
    "    return output, loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(rnn, data_tensor, seq_len=chunk_len, batch_size=BATCH_SIZE):\n",
    "    with torch.no_grad():\n",
    "        data_tensor = data_tensor.to(device)\n",
    "        hidden = rnn.init_hidden(batch_size, device=device)\n",
    "        for i in range(seq_len):\n",
    "            output, hidden = rnn(data_tensor[:,i], hidden)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "def eval_test(rnn, category_tensor, data_tensor):\n",
    "    with torch.no_grad():\n",
    "        output = evaluate(rnn, data_tensor)\n",
    "        loss = criterion(output, category_tensor.squeeze())\n",
    "        return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_iters = 1000 #2000 #100000\n",
    "print_every = 50\n",
    "plot_every = 50\n",
    "\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "current_test_loss = 0\n",
    "all_losses = []\n",
    "all_test_losses = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "number_correct = 0\n",
    "for iter in range(1, n_iters + 1):\n",
    "    input_data, target_category, text_data = load_random_batch(train_category_data, chunk_len, BATCH_SIZE)\n",
    "    output, loss = train(rnn, target_category, input_data, optimizer, criterion)\n",
    "    current_loss += loss\n",
    "    \n",
    "    _, test_loss = eval_test(rnn, target_category, input_data)\n",
    "    current_test_loss += test_loss\n",
    "    \n",
    "    guess_i = categoryFromOutput(output)\n",
    "    number_correct += (target_category.squeeze()==guess_i.squeeze()).long().sum()\n",
    "    \n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        sample_idx = 0\n",
    "        guess = all_categories[guess_i[sample_idx]]\n",
    "        \n",
    "        category = all_categories[int(target_category[sample_idx])]\n",
    "        \n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %.4f %s / %s %s' % (iter, iter / n_iters * 100, time_since(start), loss, test_loss, text_data[sample_idx], guess, correct))\n",
    "        print('Train accuracy: {}'.format(float(number_correct)/float(print_every*BATCH_SIZE)))\n",
    "        number_correct = 0\n",
    "    \n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0\n",
    "        all_test_losses.append(current_test_loss / plot_every)\n",
    "        current_test_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss functions\n",
    "--------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses, color='b')\n",
    "plt.plot(all_test_losses, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate results\n",
    "-------------------\n",
    "\n",
    "We now vizualize the performance of our model by creating a confusion matrix. The ground truth languages of samples are represented by rows in the matrix while the predicted languages are represented by columns.\n",
    "\n",
    "In this evaluation we consider sequences of variable sizes rather than the fixed length sequences we used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_batch_size = 1  # needs to be set to 1 for evaluating different sequence lengths\n",
    "\n",
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_languages, n_languages)\n",
    "n_confusion = 1000\n",
    "num_correct = 0\n",
    "total = 0\n",
    "\n",
    "for i in range(n_confusion):\n",
    "    eval_chunk_len = random.randint(10, 50) # in evaluation we will look at sequences of variable sizes\n",
    "    input_data, target_category, text_data = load_random_batch(test_category_data, chunk_len=eval_chunk_len, batch_size=eval_batch_size)\n",
    "    output = evaluate(rnn, input_data, seq_len=eval_chunk_len, batch_size=eval_batch_size)\n",
    "    \n",
    "    guess_i = categoryFromOutput(output)\n",
    "    category_i = [int(target_category[idx]) for idx in range(len(target_category))]\n",
    "    for j in range(eval_batch_size):\n",
    "        category = all_categories[category_i[j]] \n",
    "        confusion[category_i[j]][guess_i[j]] += 1\n",
    "        num_correct += int(guess_i[j]==category_i[j])\n",
    "        total += 1\n",
    "\n",
    "print('Test accuracy: ', float(num_correct)/float(n_confusion*eval_batch_size))\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_languages):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pick out bright spots off the main axis that show which\n",
    "languages it guesses incorrectly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on User Input\n",
    "---------------------\n",
    "\n",
    "Now you can test your model on your own input. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(input_line, n_predictions=5):\n",
    "    print('\\n> %s' % input_line)\n",
    "    with torch.no_grad():\n",
    "        input_data = stringToTensor(input_line).long().unsqueeze(0).to(device)\n",
    "        output = evaluate(rnn, input_data, seq_len=len(input_line), batch_size=1)\n",
    "\n",
    "    # Get top N categories\n",
    "    topv, topi = output.topk(n_predictions, dim=1)\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(n_predictions):\n",
    "        topv.shape\n",
    "        topi.shape\n",
    "        value = topv[0][i].item()\n",
    "        category_index = topi[0][i].item()\n",
    "        print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "        predictions.append([value, all_categories[category_index]])\n",
    "\n",
    "predict('This is a phrase to test the model on user input')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Kaggle submission file\n",
    "\n",
    "Once you have found a good set of hyperparameters submit the output of your model on the Kaggle test file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DO NOT CHANGE KAGGLE SUBMISSION CODE ####\n",
    "import csv\n",
    "\n",
    "kaggle_test_file_path = 'language_data/kaggle_rnn_language_classification_test.txt'\n",
    "with open(kaggle_test_file_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "output_rows = []\n",
    "for i, line in enumerate(lines):\n",
    "    sample = line.rstrip()\n",
    "    sample_chunk_len = len(sample)\n",
    "    input_data = stringToTensor(sample).unsqueeze(0)\n",
    "    output = evaluate(rnn, input_data, seq_len=sample_chunk_len, batch_size=1)\n",
    "    guess_i = categoryFromOutput(output)\n",
    "    output_rows.append((str(i+1), all_categories[guess_i]))\n",
    "\n",
    "submission_file_path = 'kaggle_rnn_submission.txt'\n",
    "with open(submission_file_path, 'w') as f:\n",
    "    output_rows = [('id', 'category')] + output_rows\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(output_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
