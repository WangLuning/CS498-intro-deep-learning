{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lifes(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env, action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangluning/miniconda3/lib/python3.7/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 2.0   memory length: 204   epsilon: 1.0    steps: 204     evaluation reward: 2.0\n",
      "episode: 1   score: 2.0   memory length: 406   epsilon: 1.0    steps: 202     evaluation reward: 2.0\n",
      "episode: 2   score: 3.0   memory length: 659   epsilon: 1.0    steps: 253     evaluation reward: 2.3333333333333335\n",
      "episode: 3   score: 3.0   memory length: 914   epsilon: 1.0    steps: 255     evaluation reward: 2.5\n",
      "episode: 4   score: 2.0   memory length: 1141   epsilon: 1.0    steps: 227     evaluation reward: 2.4\n",
      "episode: 5   score: 2.0   memory length: 1345   epsilon: 1.0    steps: 204     evaluation reward: 2.3333333333333335\n",
      "episode: 6   score: 0.0   memory length: 1479   epsilon: 1.0    steps: 134     evaluation reward: 2.0\n",
      "episode: 7   score: 3.0   memory length: 1737   epsilon: 1.0    steps: 258     evaluation reward: 2.125\n",
      "episode: 8   score: 2.0   memory length: 1942   epsilon: 1.0    steps: 205     evaluation reward: 2.111111111111111\n",
      "episode: 9   score: 2.0   memory length: 2144   epsilon: 1.0    steps: 202     evaluation reward: 2.1\n",
      "episode: 10   score: 2.0   memory length: 2342   epsilon: 1.0    steps: 198     evaluation reward: 2.090909090909091\n",
      "episode: 11   score: 2.0   memory length: 2548   epsilon: 1.0    steps: 206     evaluation reward: 2.0833333333333335\n",
      "episode: 12   score: 2.0   memory length: 2752   epsilon: 1.0    steps: 204     evaluation reward: 2.076923076923077\n",
      "episode: 13   score: 0.0   memory length: 2889   epsilon: 1.0    steps: 137     evaluation reward: 1.9285714285714286\n",
      "episode: 14   score: 2.0   memory length: 3091   epsilon: 1.0    steps: 202     evaluation reward: 1.9333333333333333\n",
      "episode: 15   score: 3.0   memory length: 3347   epsilon: 1.0    steps: 256     evaluation reward: 2.0\n",
      "episode: 16   score: 2.0   memory length: 3549   epsilon: 1.0    steps: 202     evaluation reward: 2.0\n",
      "episode: 17   score: 3.0   memory length: 3795   epsilon: 1.0    steps: 246     evaluation reward: 2.0555555555555554\n",
      "episode: 18   score: 2.0   memory length: 4016   epsilon: 1.0    steps: 221     evaluation reward: 2.0526315789473686\n",
      "episode: 19   score: 2.0   memory length: 4217   epsilon: 1.0    steps: 201     evaluation reward: 2.05\n",
      "episode: 20   score: 2.0   memory length: 4427   epsilon: 1.0    steps: 210     evaluation reward: 2.0476190476190474\n",
      "episode: 21   score: 2.0   memory length: 4634   epsilon: 1.0    steps: 207     evaluation reward: 2.0454545454545454\n",
      "episode: 22   score: 0.0   memory length: 4777   epsilon: 1.0    steps: 143     evaluation reward: 1.9565217391304348\n",
      "episode: 23   score: 2.0   memory length: 4986   epsilon: 1.0    steps: 209     evaluation reward: 1.9583333333333333\n",
      "episode: 24   score: 0.0   memory length: 5117   epsilon: 1.0    steps: 131     evaluation reward: 1.88\n",
      "episode: 25   score: 2.0   memory length: 5333   epsilon: 1.0    steps: 216     evaluation reward: 1.8846153846153846\n",
      "episode: 26   score: 0.0   memory length: 5468   epsilon: 1.0    steps: 135     evaluation reward: 1.8148148148148149\n",
      "episode: 27   score: 0.0   memory length: 5608   epsilon: 1.0    steps: 140     evaluation reward: 1.75\n",
      "episode: 28   score: 0.0   memory length: 5749   epsilon: 1.0    steps: 141     evaluation reward: 1.6896551724137931\n",
      "episode: 29   score: 3.0   memory length: 6003   epsilon: 1.0    steps: 254     evaluation reward: 1.7333333333333334\n",
      "episode: 30   score: 0.0   memory length: 6142   epsilon: 1.0    steps: 139     evaluation reward: 1.6774193548387097\n",
      "episode: 31   score: 2.0   memory length: 6351   epsilon: 1.0    steps: 209     evaluation reward: 1.6875\n",
      "episode: 32   score: 2.0   memory length: 6557   epsilon: 1.0    steps: 206     evaluation reward: 1.696969696969697\n",
      "episode: 33   score: 2.0   memory length: 6761   epsilon: 1.0    steps: 204     evaluation reward: 1.7058823529411764\n",
      "episode: 34   score: 2.0   memory length: 6964   epsilon: 1.0    steps: 203     evaluation reward: 1.7142857142857142\n",
      "episode: 35   score: 2.0   memory length: 7163   epsilon: 1.0    steps: 199     evaluation reward: 1.7222222222222223\n",
      "episode: 36   score: 2.0   memory length: 7362   epsilon: 1.0    steps: 199     evaluation reward: 1.7297297297297298\n",
      "episode: 37   score: 0.0   memory length: 7498   epsilon: 1.0    steps: 136     evaluation reward: 1.6842105263157894\n",
      "episode: 38   score: 3.0   memory length: 7758   epsilon: 1.0    steps: 260     evaluation reward: 1.7179487179487178\n",
      "episode: 39   score: 2.0   memory length: 7961   epsilon: 1.0    steps: 203     evaluation reward: 1.725\n",
      "episode: 40   score: 0.0   memory length: 8093   epsilon: 1.0    steps: 132     evaluation reward: 1.6829268292682926\n",
      "episode: 41   score: 3.0   memory length: 8349   epsilon: 1.0    steps: 256     evaluation reward: 1.7142857142857142\n",
      "episode: 42   score: 3.0   memory length: 8601   epsilon: 1.0    steps: 252     evaluation reward: 1.744186046511628\n",
      "episode: 43   score: 2.0   memory length: 8810   epsilon: 1.0    steps: 209     evaluation reward: 1.75\n",
      "episode: 44   score: 3.0   memory length: 9057   epsilon: 1.0    steps: 247     evaluation reward: 1.7777777777777777\n",
      "episode: 45   score: 0.0   memory length: 9192   epsilon: 1.0    steps: 135     evaluation reward: 1.7391304347826086\n",
      "episode: 46   score: 2.0   memory length: 9396   epsilon: 1.0    steps: 204     evaluation reward: 1.7446808510638299\n",
      "episode: 47   score: 3.0   memory length: 9643   epsilon: 1.0    steps: 247     evaluation reward: 1.7708333333333333\n",
      "episode: 48   score: 2.0   memory length: 9852   epsilon: 1.0    steps: 209     evaluation reward: 1.7755102040816326\n",
      "episode: 49   score: 0.0   memory length: 9980   epsilon: 1.0    steps: 128     evaluation reward: 1.74\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7efa201a7dce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Start training after random sample generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mtrain_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_policy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;31m# Update the target network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mUpdate_target_network_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/grad courses/498DL/Assignment5/agent.py\u001b[0m in \u001b[0;36mtrain_policy_net\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# Compute Q function of next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m### CODE ####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mnext_Q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;31m# Find maximum Q-value of action at next state from target net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m### CODE ####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "        if render_breakout:\n",
    "            env.render()\n",
    "\n",
    "        # Select and perform an action\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.) - 1\n",
    "        next_state, reward, done, info = env.step(action + 1)\n",
    "\n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['ale.lives'])\n",
    "\n",
    "        life = info['ale.lives']\n",
    "        r = np.clip(reward, -1, 1)\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network\n",
    "            if(frame % Update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "\n",
    "        if frame % 50000 == 0:\n",
    "            print('now time : ', datetime.now())\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\")\n",
    "\n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 10 episode is bigger than 400\n",
    "            # stop training\n",
    "            if np.mean(evaluation_reward) > 10:\n",
    "                torch.save(agent.model, \"./save_model/breakout_dqn\")\n",
    "                sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
